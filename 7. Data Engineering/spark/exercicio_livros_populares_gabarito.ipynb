{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício: livros populares \n",
    "\n",
    "Dado um dataset com avaliações de livros, vamos encontrar os mais populares por dois critérios:\n",
    "\n",
    "1. pela quantidade de avaliações\n",
    "\n",
    "2. pela média da nota da avaliação\n",
    "\n",
    "Motivação: construir um recomendador de livros populares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando o dataset\n",
    "\n",
    "Vamos usar o dataset `goodbooks-10k` criado para ser usado em problemas de recomendação. Ele contém cerca de 6 milhões de avaliações para os 10 mil livros mais populares.\n",
    "\n",
    "Leia mais no [fast-ml](http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/).\n",
    "\n",
    "Se tiver erro na execução da célula abaixo, baixe manualmente os arquivos do [github](https://github.com/zygmuntz/goodbooks-10k) e coloque os arquivos `books.csv` e `ratings.csv` em uma pasta chamada `data` neste diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p data\n",
    "# !wget -P data https://github.com/zygmuntz/goodbooks-10k/raw/master/books.csv\n",
    "# !wget -P data https://github.com/zygmuntz/goodbooks-10k/raw/master/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data' # se local\n",
    "#path = 'dbfs:/FileStore/tables' # se usar notebook databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = spark.read.csv(f'{path}/books.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+---------------+--------------------+----------------+--------------------+-------------+--------------+-------------+------------------+--------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|    _c0|              _c1|         _c2|    _c3|        _c4|      _c5|              _c6|            _c7|                 _c8|             _c9|                _c10|         _c11|          _c12|         _c13|              _c14|                _c15|     _c16|     _c17|     _c18|     _c19|     _c20|                _c21|                _c22|\n",
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+---------------+--------------------+----------------+--------------------+-------------+--------------+-------------+------------------+--------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|book_id|goodreads_book_id|best_book_id|work_id|books_count|     isbn|           isbn13|        authors|original_publicat...|  original_title|               title|language_code|average_rating|ratings_count|work_ratings_count|work_text_reviews...|ratings_1|ratings_2|ratings_3|ratings_4|ratings_5|           image_url|     small_image_url|\n",
      "|      1|          2767052|     2767052|2792775|        272|439023483|9.78043902348e+12|Suzanne Collins|              2008.0|The Hunger Games|The Hunger Games ...|          eng|          4.34|      4780653|           4942365|              155254|    66715|   127936|   560092|  1481305|  2706317|https://images.gr...|https://images.gr...|\n",
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+---------------+--------------------+----------------+--------------------+-------------+--------------+-------------+------------------+--------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que aconteceu aqui?\n",
    "\n",
    "Parece que os nomes das colunas não foram lidos propriamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    :param path: string, or list of strings, for input path(s),\n",
      "                 or RDD of Strings storing CSV rows.\n",
      "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "                   or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    :param sep: sets a single character as a separator for each field and value.\n",
      "                If None is set, it uses the default value, ``,``.\n",
      "    :param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
      "                     it uses the default value, ``UTF-8``.\n",
      "    :param quote: sets a single character used for escaping quoted values where the\n",
      "                  separator can be part of the value. If None is set, it uses the default\n",
      "                  value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      "                  empty string.\n",
      "    :param escape: sets a single character used for escaping quotes inside an already\n",
      "                   quoted value. If None is set, it uses the default value, ``\\``.\n",
      "    :param comment: sets a single character used for skipping lines beginning with this\n",
      "                    character. By default (None), it is disabled.\n",
      "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
      "                   default value, ``false``.\n",
      "    :param inferSchema: infers the input schema automatically from data. It requires one extra\n",
      "                   pass over the data. If None is set, it uses the default value, ``false``.\n",
      "    :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
      "                          forcibly applied to datasource files, and headers in CSV files will be\n",
      "                          ignored. If the option is set to ``false``, the schema will be\n",
      "                          validated against all headers in CSV files or the first header in RDD\n",
      "                          if the ``header`` option is set to ``true``. Field names in the schema\n",
      "                          and column names in CSV headers are checked by their positions\n",
      "                          taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      "                          ``true`` is used by default. Though the default value is ``true``,\n",
      "                          it is recommended to disable the ``enforceSchema`` option\n",
      "                          to avoid incorrect results.\n",
      "    :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
      "                                    values being read should be skipped. If None is set, it\n",
      "                                    uses the default value, ``false``.\n",
      "    :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
      "                                     values being read should be skipped. If None is set, it\n",
      "                                     uses the default value, ``false``.\n",
      "    :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      "                      the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      "                      applies to all supported types including the string type.\n",
      "    :param nanValue: sets the string representation of a non-number value. If None is set, it\n",
      "                     uses the default value, ``NaN``.\n",
      "    :param positiveInf: sets the string representation of a positive infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param negativeInf: sets the string representation of a negative infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at ``java.text.SimpleDateFormat``. This\n",
      "                       applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      "                            formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      "    :param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
      "                       set, it uses the default value, ``20480``.\n",
      "    :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
      "                              value being read. If None is set, it uses the default value,\n",
      "                              ``-1`` meaning unlimited length.\n",
      "    :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
      "                                        If specified, it is ignored.\n",
      "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      "                 set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
      "                 parse only required columns in CSV under column pruning. Therefore, corrupt\n",
      "                 records can be different based on required set of fields. This behavior can\n",
      "                 be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
      "                 (enabled by default).\n",
      "    \n",
      "            * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n",
      "              into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n",
      "              fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      "              field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      "              schema does not have the field, it drops corrupt records during parsing. \\\n",
      "              A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      "              When it meets a record having fewer tokens than the length of the schema, \\\n",
      "              sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      "              length of the schema, it drops extra tokens.\n",
      "            * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
      "            * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
      "    \n",
      "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      "                                      created by ``PERMISSIVE`` mode. This overrides\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "                                      it uses the value specified in\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    :param multiLine: parse records, which may span multiple lines. If None is\n",
      "                      set, it uses the default value, ``false``.\n",
      "    :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
      "                                      the quote character. If None is set, the default value is\n",
      "                                      escape character when escape and quote characters are\n",
      "                                      different, ``\\0`` otherwise.\n",
      "    :param samplingRatio: defines fraction of rows used for schema inferring.\n",
      "                          If None is set, it uses the default value, ``1.0``.\n",
      "    :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
      "                       the default value, empty string.\n",
      "    \n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrigindo o problema\n",
    "\n",
    "Se olharmos a documentação com calma, vamos ver esse pedaço:\n",
    "\n",
    "```\n",
    "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
    "                   default value, ``false``.\n",
    "```\n",
    "\n",
    "**Exercício:** Inclua esses parâmetro (para lermos a primeira linha como `header`) e efetue novamente a leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = spark.read.csv(f'{path}/books.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+-------+-----------+---------+----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|book_id|goodreads_book_id|best_book_id|work_id|books_count|     isbn|          isbn13|             authors|original_publication_year|      original_title|               title|language_code|average_rating|ratings_count|work_ratings_count|work_text_reviews_count|ratings_1|ratings_2|ratings_3|ratings_4|ratings_5|           image_url|     small_image_url|\n",
      "+-------+-----------------+------------+-------+-----------+---------+----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|      1|          2767052|     2767052|2792775|        272|439023483|9.78043902348E12|     Suzanne Collins|                   2008.0|    The Hunger Games|The Hunger Games ...|          eng|          4.34|      4780653|           4942365|                 155254|  66715.0|   127936|   560092|  1481305|  2706317|https://images.gr...|https://images.gr...|\n",
      "|      2|                3|           3|4640799|        491|439554934|9.78043955493E12|J.K. Rowling, Mar...|                   1997.0|Harry Potter and ...|Harry Potter and ...|          eng|          4.44|      4602479|           4800065|                  75867|  75504.0|   101676|   455024|  1156318|  3011543|https://images.gr...|https://images.gr...|\n",
      "+-------+-----------------+------------+-------+-----------+---------+----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** Agora, faça a leitura do arquivo `data/ratings.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38959)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-730349cf5bcb>\", line 1, in <module>\n",
      "    ratings_df = spark.read.csv(f'{path}/ratings.csv', inferSchema=True, header=True)\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/pyspark/sql/readwriter.py\", line 476, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o50.csv\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:38959)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-730349cf5bcb>\", line 1, in <module>\n",
      "    ratings_df = spark.read.csv(f'{path}/ratings.csv', inferSchema=True, header=True)\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/pyspark/sql/readwriter.py\", line 476, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/protocol.py\", line 336, in get_return_value\n",
      "    format(target_id, \".\", name))\n",
      "py4j.protocol.Py4JError: An error occurred while calling o50.csv\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'Py4JError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cin/conda/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o50.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-730349cf5bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mratings_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/ratings.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/conda/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o50.csv"
     ]
    }
   ],
   "source": [
    "ratings_df = spark.read.csv(f'{path}/ratings.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É comum encontrar outros formatos de dados ao trabalhar com Spark. Saiba mais [aqui](https://eng.uber.com/hdfs-file-format-apache-spark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dica para os próximos exercícios\n",
    "\n",
    "Para as próximas manipulações, é provável que usemos métodos do módulo `pyspark.sql.functions`, assim vamos importá-lo, juntamente com o módulo `pyspark.sql.types`.\n",
    "\n",
    "A documentação do módulo `pyspark.sql` pode ser encontrada [neste endereço](https://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecionando os tipos de cada coluna\n",
    "\n",
    "É possível verificar o chamado `schema` do dataframe usando o método `printSchema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que, se não tivéssemos passado o parâmetro `inferSchema=True`, então, os tipos das colunas seriam diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv('data/ratings.csv', header=True).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso caso, o Spark conseguiu inferir corretamente os tipos de cada uma das colunas, mas caso ele não tivesse conseguido, poderíamos tentar forçar a conversão através do uso da função [cast](https://spark.apache.org/docs/2.4.4/api/python/_modules/pyspark/sql/column.html#Column.cast).\n",
    "\n",
    "Por exemplo, para transformar a coluna `string` em inteiro, faríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv('data/ratings.csv', header=True).select('user_id', 'book_id', F.col('rating').cast('int')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: quantidade de avaliações de cada um dos livros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_reviews_df = ratings_df \\\n",
    "    .select('book_id', 'rating') \\\n",
    "    .groupBy('book_id') \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_reviews_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maneiras alternativas para fazer o mesmo cálculo\n",
    "\n",
    "```python\n",
    "count_reviews_df = ratings_df \\\n",
    "    .groupBy('book_id') \\\n",
    "    .agg(F.count('rating').alias('count'))\n",
    "```\n",
    "\n",
    "Também é possível usar Spark SQL:\n",
    "```python\n",
    "ratings_df.createOrReplaceTempView('ratings')\n",
    "\n",
    "count_reviews_query = \"\"\"\n",
    "    select book_id,\n",
    "        count(rating) as count\n",
    "    from ratings\n",
    "    group by book_id\n",
    "\"\"\"\n",
    "\n",
    "count_reviews_df = spark.sql(count_reviews_query)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: média das notas de avaliações de cada um dos livros\n",
    "\n",
    "**Exercício:** calcule essa média a partir do dataframe `ratings_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings_df = ratings_df \\\n",
    "    .select('book_id', 'rating') \\\n",
    "    .groupBy('book_id') \\\n",
    "    .agg(F.avg('rating').alias('mean_rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** faça o mesmo cálculo usando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.createOrReplaceTempView('ratings') # se você já rodou essa linha anteriormente, não é necessário rodá-la novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings_query = \"\"\"\n",
    "    select book_id,\n",
    "        avg(rating) as mean_rating\n",
    "    from ratings\n",
    "    group by book_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(avg_ratings_query).show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que `spark.sql(avg_ratings_query)` é um Spark DataFrame, assim como `avg_ratings_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3: junção dos dados de quantidade de avaliações e médias das notas das avaliações\n",
    "\n",
    "Queremos um dataframe que contenha `book_id`, `count` e `mean_rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df = count_reviews_df \\\n",
    "    .join(avg_ratings_df, on='book_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: junção dos dados de título e imagem\n",
    "\n",
    "Gostaríamos agora de incluir no dataframe `count_avg_ratings_df` as colunas `title` e `image_url`.\n",
    "\n",
    "**Exercício:** faça outra operação de `join`, desta vez utilizando os dataframes `count_avg_ratings_df` e `books_df` para incluir no dataframe as colunas desejadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df = count_avg_ratings_df \\\n",
    "    .join(books_df.select('book_id', 'title', 'image_url'), on='book_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 5: visualização dos livros mais populares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ordered_window = Window.orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df = count_avg_ratings_df \\\n",
    "    .withColumn('count_rank', F.row_number().over(count_ordered_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** da mesma forma, crie uma coluna chamada `avg_rank`, que calcula o rank segundo a coluna `mean_rating`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ordered_window = Window.orderBy(F.desc('mean_rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df = count_avg_ratings_df \\\n",
    "    .withColumn('avg_rank', F.row_number().over(avg_ordered_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df.orderBy('count', ascending=False).show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df.orderBy('mean_rating', ascending=False).show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** formate a tabela de modo a ver os dez livros mais populares de acordo com cada método\n",
    "\n",
    "Ao final, sua tabela deve ser como a abaixo:\n",
    "\n",
    "|rank|according_to_count                                         |according_to_avg                                                 |\n",
    "|----|-----------------------------------------------------------|-----------------------------------------------------------------|\n",
    "|1   |The Hunger Games (The Hunger Games, #1)                    |The Complete Calvin and Hobbes                                   |\n",
    "|2   |Harry Potter and the Sorcerer's Stone (Harry Potter, #1)   |ESV Study Bible                                                  |\n",
    "|3   |To Kill a Mockingbird                                      |Attack of the Deranged Mutant Killer Monster Snow Goons          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = count_avg_ratings_df.select(F.col('count_rank').alias('rank'), F.col('title').alias('according_to_count')) \\\n",
    "    .join(\n",
    "        count_avg_ratings_df.select(F.col('avg_rank').alias('rank'), F.col('title').alias('according_to_avg')),\n",
    "        on='rank') \\\n",
    "    .filter('rank <= 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: visualização das capas dos livros\n",
    "\n",
    "Para essa seção, é necessário ter instalado a biblioteca [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/user_install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout\n",
    "from IPython.display import clear_output, HTML, Markdown, display\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_imgs = count_avg_ratings_df.select(F.col('count_rank').alias('rank'), F.col('image_url').alias('according_to_count')) \\\n",
    "    .join(\n",
    "        count_avg_ratings_df.select(F.col('avg_rank').alias('rank'), F.col('image_url').alias('according_to_avg')),\n",
    "        on='rank') \\\n",
    "    .filter('rank <= 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_imgs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_imgs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_products(method, n):\n",
    "    imgs = top_10_imgs.select('rank', F.col(f'according_to_{method}').alias('url')).limit(n).collect()\n",
    "    return [(img.rank, img.url) for img in imgs]\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "def make_horizontal_box(children): return widgets.HBox(children)\n",
    "\n",
    "def make_vertical_box(children, width='auto', height='600px'):\n",
    "    return widgets.VBox(children, layout=Layout(width=width, height=height))\n",
    "\n",
    "def image_widget(url, layout=Layout(height='250px', width='150px', display='flex', align_items='center', border='solid white')):\n",
    "    img_content = requests.get(url).content\n",
    "    return widgets.Image(value=img_content, layout=layout)\n",
    "\n",
    "def widgets_to_render(method, n):\n",
    "    layout = Layout(height='250px', width='150px', display='flex', align_items='center', border='solid orchid')\n",
    "    return [image_widget(elem[1]) if i > 0 else image_widget(elem[1], layout=layout) \n",
    "            for i, elem in enumerate(get_recommended_products(method, n))]\n",
    "\n",
    "def print_on_button(string, color='lightblue'):\n",
    "    button = widgets.Button(description=string, layout=Layout(width='300px'))\n",
    "    button.style.button_color = color\n",
    "    return button\n",
    "\n",
    "def display_both_recommendations(n=3):\n",
    "    boxes = [\n",
    "        print_on_button('highest_count', color='lightblue'),\n",
    "        make_horizontal_box(widgets_to_render('count', n))\n",
    "    ]\n",
    "    boxes += [print_on_button('highest_avg', color='lightpink'),\n",
    "              make_horizontal_box(widgets_to_render('avg', n))]\n",
    "    display(make_vertical_box(boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_both_recommendations(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-kernel",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
