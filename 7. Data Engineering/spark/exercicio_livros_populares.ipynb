{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício: livros populares \n",
    "\n",
    "Dado um dataset com avaliações de livros, vamos encontrar os mais populares por dois critérios:\n",
    "\n",
    "1. pela quantidade de avaliações\n",
    "\n",
    "2. pela média da nota da avaliação\n",
    "\n",
    "Motivação: construir um recomendador de livros populares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando o dataset\n",
    "\n",
    "Vamos usar o dataset `goodbooks-10k` criado para ser usado em problemas de recomendação. Ele contém cerca de 6 milhões de avaliações para os 10 mil livros mais populares.\n",
    "\n",
    "Leia mais no [fast-ml](http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/).\n",
    "\n",
    "Se tiver erro na execução da célula abaixo, baixe manualmente os arquivos do [github](https://github.com/zygmuntz/goodbooks-10k) e coloque os arquivos `books.csv` e `ratings.csv` em uma pasta chamada `data` neste diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p data\n",
    "# !wget -P data https://github.com/zygmuntz/goodbooks-10k/raw/master/books.csv\n",
    "# !wget -P data https://github.com/zygmuntz/goodbooks-10k/raw/master/ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data' # se local\n",
    "#path = 'dbfs:/FileStore/tables' # se usar notebook databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = spark.read.csv(f'{path}/books.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que aconteceu aqui?\n",
    "\n",
    "Parece que os nomes das colunas não foram lidos propriamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrigindo o problema\n",
    "\n",
    "Se olharmos a documentação com calma, vamos ver esse pedaço:\n",
    "\n",
    "```\n",
    "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
    "                   default value, ``false``.\n",
    "```\n",
    "\n",
    "**Exercício:** Inclua esses parâmetro (para lermos a primeira linha como `header`) e efetue novamente a leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** Agora, faça a leitura do arquivo `data/ratings.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É comum encontrar outros formatos de dados ao trabalhar com Spark. Saiba mais [aqui](https://eng.uber.com/hdfs-file-format-apache-spark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dica para os próximos exercícios\n",
    "\n",
    "Para as próximas manipulações, é provável que usemos métodos do módulo `pyspark.sql.functions`, assim vamos importá-lo, juntamente com o módulo `pyspark.sql.types`.\n",
    "\n",
    "A documentação do módulo `pyspark.sql` pode ser encontrada [neste endereço](https://spark.apache.org/docs/2.4.4/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecionando os tipos de cada coluna\n",
    "\n",
    "É possível verificar o chamado `schema` do dataframe usando o método `printSchema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que, se não tivéssemos passado o parâmetro `inferSchema=True`, então, os tipos das colunas seriam diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(f'{path}/ratings.csv', header=True).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em nosso caso, o Spark conseguiu inferir corretamente os tipos de cada uma das colunas, mas caso ele não tivesse conseguido, poderíamos tentar forçar a conversão através do uso da função [cast](https://spark.apache.org/docs/2.4.4/api/python/_modules/pyspark/sql/column.html#Column.cast).\n",
    "\n",
    "Por exemplo, para transformar a coluna `string` em inteiro, faríamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(f'{path}/ratings.csv', header=True).select('user_id', 'book_id', F.col('rating').cast('int')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: quantidade de avaliações de cada um dos livros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_reviews_df = ratings_df \\\n",
    "    .select('book_id', 'rating') \\\n",
    "    .groupBy('book_id') \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_reviews_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maneiras alternativas para fazer o mesmo cálculo\n",
    "\n",
    "```python\n",
    "count_reviews_df = ratings_df \\\n",
    "    .groupBy('book_id') \\\n",
    "    .agg(F.count('rating').alias('count'))\n",
    "```\n",
    "\n",
    "Também é possível usar Spark SQL:\n",
    "```python\n",
    "ratings_df.createOrReplaceTempView('ratings')\n",
    "\n",
    "count_reviews_query = \"\"\"\n",
    "    select book_id,\n",
    "        count(rating) as count\n",
    "    from ratings\n",
    "    group by book_id\n",
    "\"\"\"\n",
    "\n",
    "count_reviews_df = spark.sql(count_reviews_query)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: média das notas de avaliações de cada um dos livros\n",
    "\n",
    "**Exercício:** calcule essa média a partir do dataframe `ratings_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings_df = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** faça o mesmo cálculo usando Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.createOrReplaceTempView('ratings') # se você já rodou essa linha anteriormente, não é necessário rodá-la novamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ratings_query = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(avg_ratings_query).show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que `spark.sql(avg_ratings_query)` é um Spark DataFrame, assim como `avg_ratings_df`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3: junção dos dados de quantidade de avaliações e médias das notas das avaliações\n",
    "\n",
    "Queremos um dataframe que contenha `book_id`, `count` e `mean_rating`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df = count_reviews_df \\\n",
    "    .join(avg_ratings_df, on='book_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: junção dos dados de título e imagem\n",
    "\n",
    "Gostaríamos agora de incluir no dataframe `count_avg_ratings_df` as colunas `title` e `image_url`.\n",
    "\n",
    "**Exercício:** faça outra operação de `join`, desta vez utilizando os dataframes `count_avg_ratings_df` e `books_df` para incluir no dataframe as colunas desejadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 5: visualização dos livros mais populares "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ordered_window = Window.orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df = count_avg_ratings_df \\\n",
    "    .withColumn('count_rank', F.row_number().over(count_ordered_window))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** da mesma forma, crie uma coluna chamada `avg_rank`, que calcula o rank segundo a coluna `mean_rating`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ordered_window = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df.orderBy('count', ascending=False).show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_avg_ratings_df.orderBy('mean_rating', ascending=False).show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** formate a tabela de modo a ver os dez livros mais populares de acordo com cada método\n",
    "\n",
    "Ao final, sua tabela deve ser como a abaixo:\n",
    "\n",
    "|rank|according_to_count                                         |according_to_avg                                                 |\n",
    "|----|-----------------------------------------------------------|-----------------------------------------------------------------|\n",
    "|1   |The Hunger Games (The Hunger Games, #1)                    |The Complete Calvin and Hobbes                                   |\n",
    "|2   |Harry Potter and the Sorcerer's Stone (Harry Potter, #1)   |ESV Study Bible                                                  |\n",
    "|3   |To Kill a Mockingbird                                      |Attack of the Deranged Mutant Killer Monster Snow Goons          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: visualização das capas dos livros\n",
    "\n",
    "Para essa seção, é necessário ter instalado a biblioteca [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/user_install.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets, Layout\n",
    "from IPython.display import clear_output, HTML, Markdown, display\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_imgs = count_avg_ratings_df.select(F.col('count_rank').alias('rank'), F.col('image_url').alias('according_to_count')) \\\n",
    "    .join(\n",
    "        count_avg_ratings_df.select(F.col('avg_rank').alias('rank'), F.col('image_url').alias('according_to_avg')),\n",
    "        on='rank') \\\n",
    "    .filter('rank <= 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_imgs.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_imgs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommended_products(method, n):\n",
    "    imgs = top_10_imgs.select('rank', F.col(f'according_to_{method}').alias('url')).limit(n).collect()\n",
    "    return [(img.rank, img.url) for img in imgs]\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "def make_horizontal_box(children): return widgets.HBox(children)\n",
    "\n",
    "def make_vertical_box(children, width='auto', height='600px'):\n",
    "    return widgets.VBox(children, layout=Layout(width=width, height=height))\n",
    "\n",
    "def image_widget(url, layout=Layout(height='250px', width='150px', display='flex', align_items='center', border='solid white')):\n",
    "    img_content = requests.get(url).content\n",
    "    return widgets.Image(value=img_content, layout=layout)\n",
    "\n",
    "def widgets_to_render(method, n):\n",
    "    layout = Layout(height='250px', width='150px', display='flex', align_items='center', border='solid orchid')\n",
    "    return [image_widget(elem[1]) if i > 0 else image_widget(elem[1], layout=layout) \n",
    "            for i, elem in enumerate(get_recommended_products(method, n))]\n",
    "\n",
    "def print_on_button(string, color='lightblue'):\n",
    "    button = widgets.Button(description=string, layout=Layout(width='300px'))\n",
    "    button.style.button_color = color\n",
    "    return button\n",
    "\n",
    "def display_both_recommendations(n=3):\n",
    "    boxes = [\n",
    "        print_on_button('highest_count', color='lightblue'),\n",
    "        make_horizontal_box(widgets_to_render('count', n))\n",
    "    ]\n",
    "    boxes += [print_on_button('highest_avg', color='lightpink'),\n",
    "              make_horizontal_box(widgets_to_render('avg', n))]\n",
    "    display(make_vertical_box(boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_both_recommendations(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-kernel",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
