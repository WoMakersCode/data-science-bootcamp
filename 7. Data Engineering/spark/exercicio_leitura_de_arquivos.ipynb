{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte prática - input de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faça o download dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-11-02 18:24:56--  https://github.com/zygmuntz/goodbooks-10k/raw/master/books.csv\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/books.csv [following]\n",
      "--2019-11-02 18:24:57--  https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/books.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.92.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.92.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3286659 (3,1M) [text/plain]\n",
      "Saving to: ‘data/books.csv’\n",
      "\n",
      "books.csv           100%[===================>]   3,13M  4,33MB/s    in 0,7s    \n",
      "\n",
      "2019-11-02 18:25:00 (4,33 MB/s) - ‘data/books.csv’ saved [3286659/3286659]\n",
      "\n",
      "--2019-11-02 18:25:00--  https://github.com/zygmuntz/goodbooks-10k/raw/master/ratings.csv\n",
      "Resolving github.com (github.com)... 140.82.113.3\n",
      "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/ratings.csv [following]\n",
      "--2019-11-02 18:25:00--  https://raw.githubusercontent.com/zygmuntz/goodbooks-10k/master/ratings.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.92.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.92.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 72126826 (69M) [text/plain]\n",
      "Saving to: ‘data/ratings.csv’\n",
      "\n",
      "ratings.csv         100%[===================>]  68,79M  5,82MB/s    in 9,6s    \n",
      "\n",
      "2019-11-02 18:25:13 (7,15 MB/s) - ‘data/ratings.csv’ saved [72126826/72126826]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget -P data https://github.com/zygmuntz/goodbooks-10k/raw/master/books.csv\n",
    "!wget -P data https://github.com/zygmuntz/goodbooks-10k/raw/master/ratings.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora, vamos ler os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = spark.read.csv('data/books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+---------------+--------------------+----------------+--------------------+-------------+--------------+-------------+------------------+--------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|    _c0|              _c1|         _c2|    _c3|        _c4|      _c5|              _c6|            _c7|                 _c8|             _c9|                _c10|         _c11|          _c12|         _c13|              _c14|                _c15|     _c16|     _c17|     _c18|     _c19|     _c20|                _c21|                _c22|\n",
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+---------------+--------------------+----------------+--------------------+-------------+--------------+-------------+------------------+--------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|book_id|goodreads_book_id|best_book_id|work_id|books_count|     isbn|           isbn13|        authors|original_publicat...|  original_title|               title|language_code|average_rating|ratings_count|work_ratings_count|work_text_reviews...|ratings_1|ratings_2|ratings_3|ratings_4|ratings_5|           image_url|     small_image_url|\n",
      "|      1|          2767052|     2767052|2792775|        272|439023483|9.78043902348e+12|Suzanne Collins|              2008.0|The Hunger Games|The Hunger Games ...|          eng|          4.34|      4780653|           4942365|              155254|    66715|   127936|   560092|  1481305|  2706317|https://images.gr...|https://images.gr...|\n",
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+---------------+--------------------+----------------+--------------------+-------------+--------------+-------------+------------------+--------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que aconteceu aqui?\n",
    "\n",
    "Parece que os nomes das colunas não foram lidos propriamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    :param path: string, or list of strings, for input path(s),\n",
      "                 or RDD of Strings storing CSV rows.\n",
      "    :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "                   or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    :param sep: sets a single character as a separator for each field and value.\n",
      "                If None is set, it uses the default value, ``,``.\n",
      "    :param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
      "                     it uses the default value, ``UTF-8``.\n",
      "    :param quote: sets a single character used for escaping quoted values where the\n",
      "                  separator can be part of the value. If None is set, it uses the default\n",
      "                  value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      "                  empty string.\n",
      "    :param escape: sets a single character used for escaping quotes inside an already\n",
      "                   quoted value. If None is set, it uses the default value, ``\\``.\n",
      "    :param comment: sets a single character used for skipping lines beginning with this\n",
      "                    character. By default (None), it is disabled.\n",
      "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
      "                   default value, ``false``.\n",
      "    :param inferSchema: infers the input schema automatically from data. It requires one extra\n",
      "                   pass over the data. If None is set, it uses the default value, ``false``.\n",
      "    :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
      "                          forcibly applied to datasource files, and headers in CSV files will be\n",
      "                          ignored. If the option is set to ``false``, the schema will be\n",
      "                          validated against all headers in CSV files or the first header in RDD\n",
      "                          if the ``header`` option is set to ``true``. Field names in the schema\n",
      "                          and column names in CSV headers are checked by their positions\n",
      "                          taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      "                          ``true`` is used by default. Though the default value is ``true``,\n",
      "                          it is recommended to disable the ``enforceSchema`` option\n",
      "                          to avoid incorrect results.\n",
      "    :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
      "                                    values being read should be skipped. If None is set, it\n",
      "                                    uses the default value, ``false``.\n",
      "    :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
      "                                     values being read should be skipped. If None is set, it\n",
      "                                     uses the default value, ``false``.\n",
      "    :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      "                      the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      "                      applies to all supported types including the string type.\n",
      "    :param nanValue: sets the string representation of a non-number value. If None is set, it\n",
      "                     uses the default value, ``NaN``.\n",
      "    :param positiveInf: sets the string representation of a positive infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param negativeInf: sets the string representation of a negative infinity value. If None\n",
      "                        is set, it uses the default value, ``Inf``.\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at ``java.text.SimpleDateFormat``. This\n",
      "                       applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      "                            formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      "    :param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
      "                       set, it uses the default value, ``20480``.\n",
      "    :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
      "                              value being read. If None is set, it uses the default value,\n",
      "                              ``-1`` meaning unlimited length.\n",
      "    :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
      "                                        If specified, it is ignored.\n",
      "    :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      "                 set, it uses the default value, ``PERMISSIVE``.\n",
      "    \n",
      "            * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n",
      "              into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n",
      "              fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      "              field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      "              schema does not have the field, it drops corrupt records during parsing. \\\n",
      "              A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      "              When it meets a record having fewer tokens than the length of the schema, \\\n",
      "              sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      "              length of the schema, it drops extra tokens.\n",
      "            * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
      "            * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
      "    \n",
      "    :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      "                                      created by ``PERMISSIVE`` mode. This overrides\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      "                                      it uses the value specified in\n",
      "                                      ``spark.sql.columnNameOfCorruptRecord``.\n",
      "    :param multiLine: parse records, which may span multiple lines. If None is\n",
      "                      set, it uses the default value, ``false``.\n",
      "    :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
      "                                      the quote character. If None is set, the default value is\n",
      "                                      escape character when escape and quote characters are\n",
      "                                      different, ``\\0`` otherwise.\n",
      "    :param samplingRatio: defines fraction of rows used for schema inferring.\n",
      "                          If None is set, it uses the default value, ``1.0``.\n",
      "    :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
      "                       the default value, empty string.\n",
      "    \n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    \n",
      "    .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrigindo o problema\n",
    "\n",
    "Se olharmos a documentação com calma, vamos ver esse pedaço:\n",
    "\n",
    "```\n",
    "    :param header: uses the first line as names of columns. If None is set, it uses the\n",
    "                   default value, ``false``.\n",
    "```\n",
    "\n",
    "**Exercício:** Inclua esse parâmetro e cheque se funcionou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = spark.read.csv('data/books.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|book_id|goodreads_book_id|best_book_id|work_id|books_count|     isbn|           isbn13|             authors|original_publication_year|      original_title|               title|language_code|average_rating|ratings_count|work_ratings_count|work_text_reviews_count|ratings_1|ratings_2|ratings_3|ratings_4|ratings_5|           image_url|     small_image_url|\n",
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "|      1|          2767052|     2767052|2792775|        272|439023483|9.78043902348e+12|     Suzanne Collins|                   2008.0|    The Hunger Games|The Hunger Games ...|          eng|          4.34|      4780653|           4942365|                 155254|    66715|   127936|   560092|  1481305|  2706317|https://images.gr...|https://images.gr...|\n",
      "|      2|                3|           3|4640799|        491|439554934|9.78043955493e+12|J.K. Rowling, Mar...|                   1997.0|Harry Potter and ...|Harry Potter and ...|          eng|          4.44|      4602479|           4800065|                  75867|    75504|   101676|   455024|  1156318|  3011543|https://images.gr...|https://images.gr...|\n",
      "+-------+-----------------+------------+-------+-----------+---------+-----------------+--------------------+-------------------------+--------------------+--------------------+-------------+--------------+-------------+------------------+-----------------------+---------+---------+---------+---------+---------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** Agora, faça a leitura do arquivo `data/ratings.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = spark.read.csv('data/ratings.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|book_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|    258|     5|\n",
      "|      2|   4081|     4|\n",
      "+-------+-------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É comum encontrar outros formatos de dados ao trabalhar com Spark. Saiba mais [aqui](https://eng.uber.com/hdfs-file-format-apache-spark/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-kernel",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
