{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício: tags que mais co-ocorrem\n",
    "\n",
    "Dado um dataset com avaliações de usuários sobre livros, vamos checar que tags co-ocorrem.\n",
    "\n",
    "Motivação: ser capaz de dizer que tags têm afinidade entre si."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixando o dataset\n",
    "\n",
    "Vamos continuar usando o dataset `goodbooks-10k` criado para ser usado em problemas de recomendação. Só para lembrar, ele contém cerca de 6 milhões de avaliações para os 10 mil livros mais populares.\n",
    "\n",
    "Leia mais no [fast-ml](http://fastml.com/goodbooks-10k-a-new-dataset-for-book-recommendations/).\n",
    "\n",
    "Dessa vez, vamos baixar os arquivos `book_tags.csv` e `tags.csv`.\n",
    "\n",
    "Novamente, se tiver erro na execução da célula abaixo, baixe manualmente os arquivos do [github](https://github.com/zygmuntz/goodbooks-10k) e coloque os arquivos `book_tags.csv` e `tags.csv` em uma pasta chamada `data` neste diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -P data https://github.com/zygmuntz/goodbooks-10k/raw/master/book_tags.csv\n",
    "# !wget -P data https://github.com/zygmuntz/goodbooks-10k/raw/master/tags.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data' # se local\n",
    "#path = 'dbfs:/FileStore/tables' # se usar notebook databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(filename):\n",
    "    return spark.read.csv(f'{path}/{filename}', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = read('books.csv')\n",
    "ratings_df = read('ratings.csv')\n",
    "book_tags_df = read('book_tags.csv')\n",
    "tags_df = read('tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remoção de tags que não desejamos no dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = tags_df.filter('tag_name not rlike \"(read|own|favorit)\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrutura dos dataframes `book_tags_df` e `tags_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- goodreads_book_id: integer (nullable = true)\n",
      " |-- tag_id: integer (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_tags_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tag_id: integer (nullable = true)\n",
      " |-- tag_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tags_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: tag mais popular para cada livro\n",
    "\n",
    "**Exercício:** una os dataframes `books_tags_df` e `tags_df`. Atenção: pela estrutura acima, é possível saber qual é a coluna que deve ser utilizada na operação de `join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_df = book_tags_df \\\n",
    "    .join(tags_df, on='tag_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** filtre o dataset para que ele só permita linhas em que a coluna `tag_name` tenha tamanho estritamente maior do que 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_df = book_tags_df \\\n",
    "    .filter('LENGTH(tag_name) > 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, basta ranquearmos as tags de acordo com sua popularidade para cada livro. Para isso, vamos usar novamente uma `Window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('goodreads_book_id').orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_df = book_tags_df \\\n",
    "    .withColumn('rank', F.row_number().over(w)) \\\n",
    "    .filter('rank = 1') \\\n",
    "    .drop('rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----+-----------------+\n",
      "|tag_id|goodreads_book_id|count|         tag_name|\n",
      "+------+-----------------+-----+-----------------+\n",
      "|  7563|             1591|  410|         clàssics|\n",
      "| 11743|             2122| 3692|          fiction|\n",
      "|  7116|             2142|   56|christian-fiction|\n",
      "|  7457|             4900| 8128|         classics|\n",
      "| 11305|             7993| 1290|          fantasy|\n",
      "+------+-----------------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_tags_df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: remoção de acentos\n",
    "\n",
    "É possível ver que algumas tags têm acentos e gostaríamos de removê-los. Para isso, vamos utilizar uma função customizada, a chamada `UDF` no Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "    return normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** crie a UDF para aplicarmos a função `strip_accents`.\n",
    "\n",
    "Dica: veja um exemplo [aqui](https://gist.github.com/zoltanctoth/2deccd69e3d1cde1dd78).\n",
    "\n",
    "Note que, em nosso caso, já temos importados os módulos `pyspark.sql.functions` como `F` e `pyspark.sql.types` como `T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_accents_udf = F.udf(strip_accents, T.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** sobrescreva a coluna `tag_name` aplicando a UDF `strip_accents_udf` nela mesma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_df = book_tags_df \\\n",
    "    .withColumn('tag_name', strip_accents_udf('tag_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+-----+--------+\n",
      "|tag_id|goodreads_book_id|count|tag_name|\n",
      "+------+-----------------+-----+--------+\n",
      "|  7563|             1591|  410|classics|\n",
      "| 11743|             2122| 3692| fiction|\n",
      "+------+-----------------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "book_tags_df.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: identificação dos livros avaliados pelas tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execício:** adicione a coluna `book_id` no dataframe `book_tags_df`.\n",
    "\n",
    "Dica: faça uma operação de `join` com o dataframe `books_df` usando como chave (o argumento do parâmetro `on`) uma coluna que ambos os dataframes têm em comum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_tags_df = book_tags_df \\\n",
    "    .join(books_df.select('book_id', 'goodreads_book_id') , on='goodreads_book_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execício:** crie um novo dataframe `user_ratings_tags_df`, que é o dataframe `ratings_df`, com a adição da coluna `tag_name`.\n",
    "\n",
    "Dica: faça uma operação de `join` com o dataframe `book_tags_df` usando como chave (o argumento do parâmetro `on`) a coluna `book_id`. Note que queremos manter o total de linhas do dataset `ratings_df`, assim, é aconselhado que o método do join (parâmetro `how`) seja `left` (caso coloque o `ratings_df` primeiro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_tags_df = ratings_df \\\n",
    "    .join(book_tags_df.select('book_id', 'tag_name'), on='book_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso um livro não tenha tags em `book_tags_df`, queremos que ele fique com a tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_tags_df = user_ratings_tags_df \\\n",
    "    .withColumn('tag_name', F.coalesce('tag_name', F.lit('unknown')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: criação dos pares de tags de cada usuário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro passo é a agregação, para cada usuário, de todas as tags.\n",
    "\n",
    "**Exercício:** agrupe o dataframe `user_ratings_tags_df` por usuário e use uma função de agregação que liste todas as `tag_name` com as quais esse usuário interagiu. Dê a essa coluna o nome de `tags`.\n",
    "\n",
    "Dica: existem duas funções que listam todos valores para cada chave de agregação: `collect_set` e `collect_list`. Neste caso, estamos interessados em uma lista com objetos únicos, assim, `collect_set` é a função mais indicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tags_df = user_ratings_tags_df \\\n",
    "    .groupBy('user_id') \\\n",
    "    .agg(F.collect_set('tag_name').alias('tags'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos montar os pares usando a função `combinations` do módulo `itertools`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(list_objs):\n",
    "    return list(combinations(sorted(list_objs), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como é uma função customizada, precisamos criar uma UDF para aplicá-la!\n",
    "\n",
    "Nesse caso, o retorno é uma lista de tuplas de `string` e, por isso, vamos criar um tipo especial para tupla.\n",
    "\n",
    "(Alternativamente, poderíamos modificar o retorno para que ele fosse lista de lista de `string`. Quer tentar?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_type = T.StructType([\n",
    "    T.StructField('pair_1', T.StringType(), nullable=False),\n",
    "    T.StructField('pair_2', T.StringType(), nullable=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** crie a UDF `make_pairs_udf`. Note que o tipo do retorno será dado por `T.ArrayType(tuple_type)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_pairs_udf = F.udf(make_pairs, T.ArrayType(tuple_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente, a chamada abaixo cria a UDF `make_pairs_df` e também registra a função para uso dentro de uma query Spark SQL.\n",
    "\n",
    "```python\n",
    "make_pairs_udf = spark.udf.register('make_pairs_udf', make_pairs, T.ArrayType(tuple_type))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** aplique `make_pairs_udf` na coluna `tags`, criando uma coluna chamada `pairs`. Ao final, delete a coluna `tags`, usando o comando `drop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = user_tags_df \\\n",
    "    .withColumn('pairs', make_pairs_udf('tags')) \\\n",
    "    .drop('tags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|               pairs|\n",
      "+-------+--------------------+\n",
      "|    148|[[africa, audiobo...|\n",
      "|    463|[[adventure, chic...|\n",
      "+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs_df.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- pairs: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- pair_1: string (nullable = false)\n",
      " |    |    |-- pair_2: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pairs_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: contagem dos pares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro passo aqui é transformar nosso dataframe, de forma que cada um dos elementos da lista contida na coluna `pairs` esteja em uma linha. Isso pode ser facilmente feito através do método `explode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_pair_per_row_df = pairs_df \\\n",
    "    .select('user_id', F.explode('pairs').alias('pair'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|pair               |\n",
      "+-------+-------------------+\n",
      "|148    |[africa, audiobook]|\n",
      "|148    |[africa, book-club]|\n",
      "+-------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_pair_per_row_df.show(n=2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** para cada par, conte a quantidade de usuários que interagiram com ele. Ao final, faça uma ordenação decrescente do valor da coluna `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_count_df = one_pair_per_row_df \\\n",
    "    .groupBy('pair') \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: os pares mais comuns são..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----+\n",
      "|pair                         |count|\n",
      "+-----------------------------+-----+\n",
      "|[fantasy, fiction]           |48531|\n",
      "|[classics, fiction]          |47076|\n",
      "|[classics, fantasy]          |44802|\n",
      "|[fiction, non-fiction]       |43409|\n",
      "|[fiction, young-adult]       |43322|\n",
      "|[fantasy, young-adult]       |42684|\n",
      "|[fantasy, non-fiction]       |40720|\n",
      "|[classics, non-fiction]      |40696|\n",
      "|[fiction, historical-fiction]|39634|\n",
      "|[classics, young-adult]      |39549|\n",
      "+-----------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pair_count_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-kernel",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
